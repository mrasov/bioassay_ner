{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"Token classification (PyTorch)","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10641480,"sourceType":"datasetVersion","datasetId":6588740}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets evaluate transformers[sentencepiece] accelerate seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T17:01:47.210367Z","iopub.execute_input":"2025-02-02T17:01:47.210709Z","iopub.status.idle":"2025-02-02T17:01:53.987991Z","shell.execute_reply.started":"2025-02-02T17:01:47.210679Z","shell.execute_reply":"2025-02-02T17:01:53.986945Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.5)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.2.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=9ae864353211b0e58c21a143502ec76f0d7f95aedbeeb05c6e5898a351a593d9\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval, evaluate\nSuccessfully installed evaluate-0.4.3 seqeval-1.2.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification, get_scheduler\nimport evaluate\nfrom accelerate import Accelerator\nimport shutil","metadata":{"id":"HfgU_LLJSKoo","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T17:01:53.989278Z","iopub.execute_input":"2025-02-02T17:01:53.989576Z","iopub.status.idle":"2025-02-02T17:02:14.344633Z","shell.execute_reply.started":"2025-02-02T17:01:53.989542Z","shell.execute_reply":"2025-02-02T17:02:14.343976Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df = pd.read_parquet('/kaggle/input/ner_data_raw.prqt')\ndf = df.sample(frac=1, random_state=111).reset_index(drop=True)\n\nlabel2id = {\n    'O': 0,\n    'B-TARGET': 1, \n    'I-TARGET': 2,\n    'B-SUBSTRATE': 3,\n    'I-SUBSTRATE': 4\n}\nid2label = {v: k for k, v in label2id.items()}\nlabel_names = list(label2id.keys())\n\ndf['tokens'] = df.ner_data.apply(lambda x: [_[0] for _ in x])\ndf['ner_tags'] = df.ner_data.apply(lambda x: [label2id[_[1]] for _ in x])\n\nraw_datasets = DatasetDict(\n    {\n    'train': Dataset.from_pandas(df[['tokens', 'ner_tags']].iloc[:7000, :]),\n    'validation': Dataset.from_pandas(df[['tokens', 'ner_tags']].iloc[7000:7500, :]),\n    'test': Dataset.from_pandas(df[['tokens', 'ner_tags']].iloc[7500:, :])\n    }\n)\n\nraw_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T17:02:14.346098Z","iopub.execute_input":"2025-02-02T17:02:14.346693Z","iopub.status.idle":"2025-02-02T17:02:14.649502Z","shell.execute_reply.started":"2025-02-02T17:02:14.346670Z","shell.execute_reply":"2025-02-02T17:02:14.648684Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'ner_tags'],\n        num_rows: 7000\n    })\n    validation: Dataset({\n        features: ['tokens', 'ner_tags'],\n        num_rows: 500\n    })\n    test: Dataset({\n        features: ['tokens', 'ner_tags'],\n        num_rows: 577\n    })\n})"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"model_checkpoint = \"michiyasunaga/BioLinkBERT-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForTokenClassification.from_pretrained(\n    model_checkpoint,\n    id2label=id2label,\n    label2id=label2id,\n)\nmodel.to('cuda')","metadata":{"id":"kiB2ZEeRSKou","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T17:02:14.650671Z","iopub.execute_input":"2025-02-02T17:02:14.650971Z","iopub.status.idle":"2025-02-02T17:02:19.763771Z","shell.execute_reply.started":"2025-02-02T17:02:14.650906Z","shell.execute_reply":"2025-02-02T17:02:19.762580Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/379 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c9cd53bc8474e6d87aaf16198129a0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/225k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"234c540339f049dfa639cdc9ea293c0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/447k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f94a90e3d6043c6828bdd593c9e9515"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46bc387a210a4125bc31be5dc91031fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/559 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"525bcc49cb8f4ac4ba476b1347692148"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d3e9906121e4cc8b404ff58c6bceb3e"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28895, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def align_labels_with_tokens(labels, word_ids):\n    new_labels = []\n    current_word = None\n    for word_id in word_ids:\n        if word_id != current_word:\n            current_word = word_id\n            label = -100 if word_id is None else labels[word_id]\n            new_labels.append(label)\n        elif word_id is None:\n            new_labels.append(-100)\n        else:\n            label = labels[word_id]\n            if label % 2 == 1:\n                label += 1\n            new_labels.append(label)\n\n    return new_labels\n\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"], truncation=True, is_split_into_words=True\n    )\n    all_labels = examples[\"ner_tags\"]\n    new_labels = []\n    for i, labels in enumerate(all_labels):\n        word_ids = tokenized_inputs.word_ids(i)\n        new_labels.append(align_labels_with_tokens(labels, word_ids))\n\n    tokenized_inputs[\"labels\"] = new_labels\n    return tokenized_inputs\n\ndef compute_metrics(eval_preds):\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n\n    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n    true_predictions = [\n        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": all_metrics[\"overall_precision\"],\n        \"recall\": all_metrics[\"overall_recall\"],\n        \"f1\": all_metrics[\"overall_f1\"],\n        \"accuracy\": all_metrics[\"overall_accuracy\"],\n    }\n\ndef postprocess(predictions, labels):\n    predictions = predictions.detach().cpu().clone().numpy()\n    labels = labels.detach().cpu().clone().numpy()\n\n    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n    true_predictions = [\n        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    return true_labels, true_predictions","metadata":{"id":"buFJQiBlSKov","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T17:02:19.765483Z","iopub.execute_input":"2025-02-02T17:02:19.766042Z","iopub.status.idle":"2025-02-02T17:02:19.776525Z","shell.execute_reply.started":"2025-02-02T17:02:19.766008Z","shell.execute_reply":"2025-02-02T17:02:19.775834Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"tokenized_datasets = raw_datasets.map(\n    tokenize_and_align_labels,\n    batched=True,\n    remove_columns=raw_datasets[\"train\"].column_names,\n)\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\nmetric = evaluate.load(\"seqeval\")","metadata":{"id":"580w11ewSKow","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T17:02:19.777330Z","iopub.execute_input":"2025-02-02T17:02:19.777689Z","iopub.status.idle":"2025-02-02T17:02:21.656810Z","shell.execute_reply.started":"2025-02-02T17:02:19.777661Z","shell.execute_reply":"2025-02-02T17:02:21.656142Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d1a47c18c3449197187cf80d5caa0f"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dac2e6c0f1c4e9495db2fca220ea1a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/577 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbd9f9879e32470aab58113bba4bd3d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25f441e929f846f2ad8ca349187f2497"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    tokenized_datasets[\"train\"],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=64,\n)\neval_dataloader = DataLoader(\n    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=64\n)\ntest_dataloader = DataLoader(\n    tokenized_datasets[\"test\"], collate_fn=data_collator, batch_size=64\n)\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\naccelerator = Accelerator()\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)\n\nnum_train_epochs = 5\nnum_update_steps_per_epoch = len(train_dataloader)\nnum_training_steps = num_train_epochs * num_update_steps_per_epoch\n\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)\n\n! mkdir linkbert_bioassay_ner\noutput_dir = \"linkbert_bioassay_ner\"","metadata":{"id":"QjYGL1QhSKo2","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T17:02:21.657675Z","iopub.execute_input":"2025-02-02T17:02:21.657886Z","iopub.status.idle":"2025-02-02T17:02:21.838780Z","shell.execute_reply.started":"2025-02-02T17:02:21.657866Z","shell.execute_reply":"2025-02-02T17:02:21.837802Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"progress_bar = tqdm(range(num_training_steps))\n\nfor epoch in range(num_train_epochs):\n\n    model.train()\n    for batch in train_dataloader:\n        outputs = model(**batch)\n        loss = outputs.loss\n        accelerator.backward(loss)\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n\n    model.eval()\n    for batch in eval_dataloader:\n        with torch.no_grad():\n            outputs = model(**batch)\n\n        predictions = outputs.logits.argmax(dim=-1)\n        labels = batch[\"labels\"]\n\n        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n\n        predictions_gathered = accelerator.gather(predictions)\n        labels_gathered = accelerator.gather(labels)\n\n        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n        metric.add_batch(predictions=true_predictions, references=true_labels)\n\n    results = metric.compute()\n    print(\n        f\"epoch {epoch}:\",\n        {\n            key: results[f\"overall_{key}\"]\n            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n        },\n    )\n\n    accelerator.wait_for_everyone()\n    unwrapped_model = accelerator.unwrap_model(model)\n    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n    if accelerator.is_main_process:\n        tokenizer.save_pretrained(output_dir)\n\naccelerator.wait_for_everyone()\nunwrapped_model = accelerator.unwrap_model(model)\nunwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)","metadata":{"id":"UyV7y2e3SKo3","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T17:02:21.841709Z","iopub.execute_input":"2025-02-02T17:02:21.841942Z","iopub.status.idle":"2025-02-02T17:08:29.682166Z","shell.execute_reply.started":"2025-02-02T17:02:21.841921Z","shell.execute_reply":"2025-02-02T17:08:29.681216Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/550 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c19bc9ebdc234c35b9d86eb1fba02e56"}},"metadata":{}},{"name":"stdout","text":"epoch 0: {'precision': 0.9138166894664843, 'recall': 0.835, 'f1': 0.8726322664924886, 'accuracy': 0.9782276268841477}\nepoch 1: {'precision': 0.9233926128590971, 'recall': 0.8522727272727273, 'f1': 0.8864084044648719, 'accuracy': 0.9778635403771936}\nepoch 2: {'precision': 0.9247606019151847, 'recall': 0.8813559322033898, 'f1': 0.9025367156208278, 'accuracy': 0.9820141265564698}\nepoch 3: {'precision': 0.9220246238030095, 'recall': 0.8730569948186528, 'f1': 0.8968729208250165, 'accuracy': 0.9812859535425618}\nepoch 4: {'precision': 0.9247606019151847, 'recall': 0.8745148771021992, 'f1': 0.898936170212766, 'accuracy': 0.9816500400495157}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"model.eval()\nfor batch in test_dataloader:\n    batch.to('cuda')\n    with torch.no_grad():\n        outputs = model(**batch)\n\n    predictions = outputs.logits.argmax(dim=-1)\n    labels = batch[\"labels\"]\n\n    predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n\n    predictions_gathered = accelerator.gather(predictions)\n    labels_gathered = accelerator.gather(labels)\n\n    true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n    metric.add_batch(predictions=true_predictions, references=true_labels)\n\nresults = metric.compute()\nprint(\n    f\"Final metric on test set:\",\n    {\n        key: results[f\"overall_{key}\"]\n        for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n    },\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T17:08:29.683351Z","iopub.execute_input":"2025-02-02T17:08:29.683576Z","iopub.status.idle":"2025-02-02T17:08:31.975572Z","shell.execute_reply.started":"2025-02-02T17:08:29.683557Z","shell.execute_reply":"2025-02-02T17:08:31.974695Z"}},"outputs":[{"name":"stdout","text":"Final metric on test set: {'precision': 0.9422632794457275, 'recall': 0.8898582333696837, 'f1': 0.9153112731351654, 'accuracy': 0.9840529648819805}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"text = 'Activation of human beta-adrenergic receptor by isoproterenol at 0.5 uM measured as increase in cAMP levels'\n\nexample = tokenizer(text, return_tensors=\"pt\").to('cuda')\n\nwith torch.no_grad():\n    logits = model(**example).logits\n\npredictions = torch.argmax(logits, dim=2)\npredicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n\ntokens = tokenizer.convert_ids_to_tokens(example['input_ids'][0])\nmax_length = max(len(token) for token in tokens)\n\nfor token, label in zip(tokens, predicted_token_class):\n    print(f\"{token: <{max_length}} {label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T17:08:31.976494Z","iopub.execute_input":"2025-02-02T17:08:31.976800Z","iopub.status.idle":"2025-02-02T17:08:31.994335Z","shell.execute_reply.started":"2025-02-02T17:08:31.976769Z","shell.execute_reply":"2025-02-02T17:08:31.993726Z"}},"outputs":[{"name":"stdout","text":"[CLS]         O\nactivation    O\nof            O\nhuman         O\nbeta          B-TARGET\n-             I-TARGET\nadrenergic    I-TARGET\nreceptor      I-TARGET\nby            O\nisoproterenol B-SUBSTRATE\nat            O\n0             O\n.             O\n5             O\num            O\nmeasured      O\nas            O\nincrease      O\nin            O\ncamp          O\nlevels        O\n[SEP]         O\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"source_directory = 'linkbert_bioassay_ner'\narchive_path = 'linkbert_bioassay_ner.zip'\nshutil.make_archive(archive_path.replace('.zip', ''), 'zip', source_directory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T17:08:31.994958Z","iopub.execute_input":"2025-02-02T17:08:31.995136Z","iopub.status.idle":"2025-02-02T17:08:52.985555Z","shell.execute_reply.started":"2025-02-02T17:08:31.995120Z","shell.execute_reply":"2025-02-02T17:08:52.984736Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/linkbert_bioassay_ner.zip'"},"metadata":{}}],"execution_count":11}]}